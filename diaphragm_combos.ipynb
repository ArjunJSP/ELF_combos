{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/CMN4T2fOQC2ZrG0J/OXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunJSP/ELF_combos/blob/main/diaphragm_combos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wONJBvka7tgv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from google.colab import data_table\n",
        "import itertools\n",
        "from itertools import permutations\n",
        "from itertools import accumulate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to a specific folder in your Google Drive\n",
        "%cd /content/drive/My Drive/mountlocation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvmL57c5JPM9",
        "outputId": "3a43925d-2176-4674-b999-71541bfe2235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/mountlocation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Files for Editing"
      ],
      "metadata": {
        "id": "aYw8lge_-q1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#e2k File to write-to (currently not using)\n",
        "# e2k_file = 'RSFH_TOWER_V4.4.14.e2k'\n",
        "\n",
        "#ETABS Excel output of story forces for RSX-ELF, RSY-ELF, and 00_SEISMASS\n",
        "file_path = \"StoryForces_V6.5.7_diaphragm_0.25.xlsx\""
      ],
      "metadata": {
        "id": "rDSAplQ0-qbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Story Forces and Static Load Case scale factors\n",
        "\n",
        "Data defined in this section include:\n",
        "*   pv_storyshears_X\n",
        "*   pv_storyshears_Y\n",
        "*   pv_seismass\n",
        "*   base_shear_coeff_VX\n",
        "*   base_shear_coeff_VY\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e_BZ4c7xICXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seismic Factors for Fpx calculation:\n",
        "Sds = 1.17  #updated Sds\n",
        "Ie = 1.5"
      ],
      "metadata": {
        "id": "LixUxsXGuSpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######evaluate units more carefully for the excel sheet - or make sure they are exported to access using the right units!\n",
        "# Read the first two rows to get headers and sub-headers\n",
        "headers = pd.read_excel(file_path, sheet_name=\"Story Forces\", header=[1])\n",
        "\n",
        "# Read the actual data, skipping the first two rows used for headers\n",
        "df_storyforces = pd.read_excel(file_path, sheet_name=\"Story Forces\", skiprows=[0, 1,2])\n",
        "\n",
        "# Set the MultiIndex columns\n",
        "df_storyforces.columns = headers.columns"
      ],
      "metadata": {
        "id": "Nfpuz68aIIZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######evaluate units more carefully for the excel sheet - or make sure they are exported to access using the right units!\n",
        "# Read the first two rows to get headers and sub-headers\n",
        "headers = pd.read_excel(file_path, sheet_name=\"Mass Summary by Story\", header=[1])\n",
        "\n",
        "# Read the actual data, skipping the first two rows used for headers\n",
        "df_masssummary = pd.read_excel(file_path, sheet_name=\"Mass Summary by Story\", skiprows=[0, 2])\n",
        "\n",
        "df_masssummary['P'] = df_masssummary['UX']*32.2/1000 #multiply by gravity and convert to kips (units must be lb-s^2/ft for mass)\n",
        "df_masssummary['P_Total'] = df_masssummary['P'].cumsum(axis = 0, skipna = True) #cumulative mass added up along building\n",
        "dropindex = df_masssummary.loc[df_masssummary['Story'] == 'L00'].index[0]\n",
        "df_masssummary = df_masssummary.drop(dropindex, axis=0)"
      ],
      "metadata": {
        "id": "pLM3Fr7_FSpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#required inputs:\n",
        "num_levels = len(df_storyforces['Story'].drop_duplicates())"
      ],
      "metadata": {
        "id": "ogGyPN3t_dY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse story force output into useable dataframes\n",
        "relevant_cols = ['Story','Output Case','Step Type','Location','VX','VY']\n",
        "pv_storyshears_X = df_storyforces[relevant_cols]\n",
        "pv_storyshears_X = pv_storyshears_X[pv_storyshears_X['Location'].isin(['Bottom'])]\n",
        "pv_storyshears_X = pv_storyshears_X[pv_storyshears_X['Step Type'].isin(['Max'])]\n",
        "pv_storyshears_X = pv_storyshears_X[pv_storyshears_X['Output Case'].isin(['RSX-STR (ELF)'])]\n",
        "\n",
        "pv_storyshears_Y = df_storyforces[relevant_cols]\n",
        "pv_storyshears_Y = pv_storyshears_Y[pv_storyshears_Y['Location'].isin(['Bottom'])]\n",
        "pv_storyshears_Y = pv_storyshears_Y[pv_storyshears_Y['Step Type'].isin(['Max'])]\n",
        "pv_storyshears_Y = pv_storyshears_Y[pv_storyshears_Y['Output Case'].isin(['RSY-STR (ELF)'])]\n",
        "\n",
        "relevant_cols = ['Story','Output Case','Location','P']\n",
        "pv_seismass = df_storyforces[relevant_cols]\n",
        "pv_seismass = pv_seismass[pv_seismass['Location'].isin(['Bottom'])]\n",
        "pv_seismass = pv_seismass[pv_seismass['Output Case'].isin(['00_SEISMIC MASS'])]"
      ],
      "metadata": {
        "id": "tn8FVapiIOkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Calculate the static base shear coefficients for X\n",
        "# base_shear_coeff_VX = pv_storyshears_X['Story'].reset_index(drop=True).to_frame()\n",
        "# x_coeff = ( pv_storyshears_X['VX'].reset_index(drop=True)-pv_storyshears_X['VX'].shift(1).fillna(0).reset_index(drop=True) ) / ( pv_seismass['P'].reset_index(drop=True)-pv_seismass['P'].shift(1).fillna(0).reset_index(drop=True) )\n",
        "# x_coeff = x_coeff.rename(\"Coeff\").to_frame()\n",
        "# base_shear_coeff_VX = base_shear_coeff_VX.merge(x_coeff, left_index=True, right_index=True)\n",
        "\n",
        "# #Calculate the static base shear coefficients for Y\n",
        "# base_shear_coeff_VY = pv_storyshears_Y['Story'].reset_index(drop=True).to_frame()\n",
        "# y_coeff = ( pv_storyshears_Y['VY'].reset_index(drop=True)-pv_storyshears_Y['VY'].shift(1).fillna(0).reset_index(drop=True) ) / ( pv_seismass['P'].reset_index(drop=True)-pv_seismass['P'].shift(1).fillna(0).reset_index(drop=True) )\n",
        "# y_coeff = y_coeff.rename(\"Coeff\").to_frame()\n",
        "# base_shear_coeff_VY = base_shear_coeff_VY.merge(y_coeff, left_index=True, right_index=True)\n"
      ],
      "metadata": {
        "id": "3rjKgdzwuRZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the static base shear coefficients for X\n",
        "base_shear_coeff_VX = pv_storyshears_X['Story'].reset_index(drop=True).to_frame()\n",
        "x_coeff = ( pv_storyshears_X['VX'].reset_index(drop=True)-pv_storyshears_X['VX'].shift(1).fillna(0).reset_index(drop=True) ) / (df_masssummary['P'])\n",
        "x_coeff = x_coeff.rename(\"Coeff\").to_frame()\n",
        "base_shear_coeff_VX = base_shear_coeff_VX.merge(x_coeff, left_index=True, right_index=True)\n",
        "\n",
        "#Calculate the static base shear coefficients for Y\n",
        "base_shear_coeff_VY = pv_storyshears_Y['Story'].reset_index(drop=True).to_frame()\n",
        "y_coeff = ( pv_storyshears_Y['VY'].reset_index(drop=True)-pv_storyshears_Y['VY'].shift(1).fillna(0).reset_index(drop=True) ) /  (df_masssummary['P'])\n",
        "y_coeff = y_coeff.rename(\"Coeff\").to_frame()\n",
        "base_shear_coeff_VY = base_shear_coeff_VY.merge(y_coeff, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "_XrbrcyHJ8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Permutation Parameters\n",
        "Generate all permutations for load combination based on the following parameters:\n",
        "- direction (X,Y)\n",
        "- level\n",
        "- eccentricity (P,N)\n"
      ],
      "metadata": {
        "id": "dvEpzbse9wsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function create parameter array for level names\n",
        "def create_l_array(n):\n",
        "    \"\"\"\n",
        "    Generate an array of text elements with the letter 'L' concatenated with numbers from 01 to n.\n",
        "    Adds a leading zero for numbers less than 10.\n",
        "\n",
        "    :param n: Length of the array.\n",
        "    :return: List of strings in the format 'L01', 'L02', ..., 'Ln'.\n",
        "    \"\"\"\n",
        "    return [f\"L{str(i).zfill(2)}\" for i in range(n, 0, -1)]\n"
      ],
      "metadata": {
        "id": "HO6ZwoXb-j56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#axis parameters\n",
        "param_axis = ['X','Y']\n",
        "\n",
        "#level parameters\n",
        "param_level = create_l_array(num_levels)\n",
        "\n",
        "#direction parameters\n",
        "param_direction = ['P','N']\n",
        "\n",
        "#eccentricity parameters\n",
        "param_ecc = ['P','N']\n",
        "ecc_percent = 0.05"
      ],
      "metadata": {
        "id": "Flh1-__K90i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to create a list of permutations\n",
        "def generate_permutations(*criteria):\n",
        "    \"\"\"\n",
        "    Generate all possible permutations from multiple criteria lists.\n",
        "\n",
        "    :param criteria: Variable number of lists containing elements for permutation.\n",
        "    :return: List of tuples representing all possible permutations.\n",
        "    \"\"\"\n",
        "    return list(itertools.product(*criteria))"
      ],
      "metadata": {
        "id": "s4jjCFfX7_mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#permutations to compute:\n",
        "# 0: axis (X,Y)\n",
        "# 1: direction (positive, negative)\n",
        "# 2: eccentricity (positive, negative)\n",
        "# 3: level\n",
        "permutations = generate_permutations(param_axis, param_direction, param_ecc, param_level)\n",
        "n = len(permutations)"
      ],
      "metadata": {
        "id": "M9cQ8xnM_KCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Fpx Factors\n",
        "\n",
        "\n",
        "*   Fpx_SF = table of scale factors for each floor, for each direction\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dg_XbFSgT3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fpx calcualation based on ASCE 7-16 12.10.1.1: Diaphragm Design Forces (RE-WRITE USING MASS SOURCE)\n",
        "\n",
        "Wpx = df_masssummary['P']\n",
        "Fx_X = (pv_storyshears_X.VX - pv_storyshears_X.VX.shift(1).fillna(0.0)).reset_index(drop=True)\n",
        "Fx_Y = (pv_storyshears_Y.VY - pv_storyshears_Y.VY.shift(1).fillna(0.0)).reset_index(drop=True)\n",
        "\n",
        "Wi = df_masssummary['P_Total']\n",
        "Fi_x = pv_storyshears_X.VX.reset_index(drop=True)\n",
        "Fi_y = pv_storyshears_Y.VY.reset_index(drop=True)\n",
        "\n",
        "Fpx_X = [None] * num_levels\n",
        "Fpx_Y = [None] * num_levels\n",
        "\n",
        "for i in range(num_levels):\n",
        "  Fpx_X[i] = Fi_x[i]/Wi[i]*Wpx[i]       # 12.10-1\n",
        "  if Fpx_X[i] < 0.2*Sds*Ie*Wpx[i]:\n",
        "    Fpx_X[i] = 0.2*Sds*Ie*Wpx[i]        # 12.10-2\n",
        "  if Fpx_X[i] > 0.4*Sds*Ie*Wpx[i]:\n",
        "    Fpx_X[i] = 0.4*Sds*Ie*Wpx[i]        # 12.10-3\n",
        "\n",
        "for i in range(num_levels):\n",
        "  Fpx_Y[i] = Fi_y[i]/Wi[i]*Wpx[i]       # 12.10-1\n",
        "  if Fpx_Y[i] < 0.2*Sds*Ie*Wpx[i]:\n",
        "    Fpx_Y[i] = 0.2*Sds*Ie*Wpx[i]        # 12.10-2\n",
        "  if Fpx_Y[i] > 0.4*Sds*Ie*Wpx[i]:\n",
        "    Fpx_Y[i] = 0.4*Sds*Ie*Wpx[i]        # 12.10-3\n",
        "\n",
        "#Story Scale Factor coefficient\n",
        "Fpx_X_SF = (Fpx_X / Fx_X).round(2)\n",
        "Fpx_Y_SF = (Fpx_Y / Fx_Y).round(2)\n",
        "\n",
        "Fpx_SF = pd.DataFrame([Fpx_X_SF,Fpx_Y_SF]) # .rename(columns=param_level)\n",
        "Fpx_SF.columns = param_level\n",
        "Fpx_SF.index = ['X','Y']"
      ],
      "metadata": {
        "id": "JKjoBPQagWZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Load Patterns\n",
        "\n",
        "*   pattern_array = names for all of the required load patterns, based on an individual pattern for each load permutation\n",
        "*   load_pattern_array = load_pattern_array1 + load_pattern_array2, where both are the component lines for the total line section that is the load pattern array\n",
        "\n"
      ],
      "metadata": {
        "id": "sRilnQvZCVyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first line in load pattern definitions\n",
        "\n",
        "pattern_array = [None] * n\n",
        "load_pattern_array1 = [None] * n\n",
        "for i in range(n):\n",
        "  pattern_array[i] = ' \"ELF-EQ'+ permutations[i][0] + permutations[i][1] + \"-\" + permutations[i][2] + \"-\" + permutations[i][3] + '\"'\n",
        "  load_pattern_array1[i] = \"  LOADPATTERN\" + pattern_array[i] + '  TYPE  \"Seismic\"  SELFWEIGHT  0'"
      ],
      "metadata": {
        "id": "Ygcms-dKH05U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#second line in load pattern definitions\n",
        "\n",
        "## create a list of length 'n' to define positive (+) or negative (-) eccentricity\n",
        "ecc_array = [None] * n\n",
        "for i in range(n):\n",
        "  if permutations[i][2] == \"P\":\n",
        "    ecc_array[i] = 'DIR \"' + permutations[i][0] + '+ECC\"  ECC ' + str(ecc_percent)\n",
        "  else:\n",
        "    ecc_array[i] = 'DIR \"' + permutations[i][0] + '-ECC\"  ECC ' + str(ecc_percent)\n",
        "\n",
        "## create a list of length 'n' to define TOPSTORY and BOTTOMSTORY\n",
        "\n",
        "df_stories = pd.concat([df_storyforces['Story'].drop_duplicates().reset_index(drop=True).to_frame().rename(columns={\"Story\": \"TOPSTORY\"}), df_storyforces['Story'].drop_duplicates().shift(-1).fillna(\"L00\").reset_index(drop=True).to_frame().rename(columns={\"Story\": \"BOTTOMSTORY\"})], axis=1)\n",
        "# df_stories =  pd.concat([df_storyforces['Story'].drop_duplicates().reset_index(drop=True).to_frame().rename(columns={\"Story\": \"TOPSTORY\"}), df_storyforces['Story'].drop_duplicates().reset_index(drop=True).to_frame().rename(columns={\"Story\": \"BOTTOMSTORY\"})], axis=1)\n",
        "\n",
        "story_array = [None] * n\n",
        "for i in range(n):\n",
        "  TOPSTORY = permutations[i][3]\n",
        "  BOTTOMSTORY = df_stories.loc[df_stories['TOPSTORY'] == TOPSTORY, 'BOTTOMSTORY'].iloc[0]\n",
        "  story_array[i] = 'TOPSTORY' + ' \"' + TOPSTORY + '\"    ' + 'BOTTOMSTORY' + ' \"' + BOTTOMSTORY + '\"    '\n",
        "\n",
        "## assign SHEARCOEFF based on TOPSTORY definition\n",
        "shear_array = [None] * n\n",
        "shear_values = [None] * n\n",
        "for i in range(n):\n",
        "  if permutations[i][0] == \"X\":\n",
        "    shear_values[i] = base_shear_coeff_VX.loc[base_shear_coeff_VX['Story'] == permutations[i][3], 'Coeff'].iloc[0].round(4)\n",
        "  else:\n",
        "    shear_values[i] = base_shear_coeff_VY.loc[base_shear_coeff_VY['Story'] == permutations[i][3], 'Coeff'].iloc[0].round(4)\n",
        "\n",
        "# !! consider adding negative sign to shear coefficient for negative directions !!\n",
        "  shear_array[i] = 'SHEARCOEFF ' + str(shear_values[i]) + '  HEIGHTEXPONENT 1'\n",
        "\n",
        "# create final array\n",
        "load_pattern_array2 = [None] * n\n",
        "\n",
        "for i in range(n):\n",
        "  load_pattern_array2[i] = '  SEISMIC ' + pattern_array[i] + '  \"User Coefficient\"    ' + ecc_array[i] + '  ' + story_array[i] + shear_array[i]\n"
      ],
      "metadata": {
        "id": "uahuBHBGKEHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_pattern_array = load_pattern_array1 +  load_pattern_array2"
      ],
      "metadata": {
        "id": "3W5FgyXC9gxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Load Cases"
      ],
      "metadata": {
        "id": "dbMoIJDOLoG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loadcase_line1_array = [None] * n\n",
        "loadcase_line2_array = [None] * n\n",
        "load_case_array = [None] * n*2\n",
        "for i in range(n):\n",
        "  loadcase_line1_array[i] = '  LOADCASE ' + pattern_array[i] + '  TYPE  \"Linear Static\"  INITCOND  \"PRESET\"  '\n",
        "  if permutations[i][1] == \"P\":\n",
        "    loadcase_line2_array[i] = '  LOADCASE ' + pattern_array[i] + '  LOADPAT  ' + pattern_array[i] + '  SF  1'\n",
        "  else:\n",
        "    loadcase_line2_array[i] = '  LOADCASE ' + pattern_array[i] + '  LOADPAT  ' + pattern_array[i] + '  SF  -1'\n",
        "\n",
        "\n",
        "for i in range(n*2):\n",
        "  load_case_array[::2] = loadcase_line1_array\n",
        "  load_case_array[1::2] = loadcase_line2_array"
      ],
      "metadata": {
        "id": "V6_Ctdf0Lpu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Utility Load Combinations"
      ],
      "metadata": {
        "id": "ggH0Ghc-EdG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Envelope all eccentricity and direction on a given floor, for both X and Y\n",
        "\n",
        "utility_combo1 = [];\n",
        "\n",
        "type = '\"Envelope\"'\n",
        "\n",
        "#permutations required for utility combo 1 capturing positive/negative direction and eccentricity for a given axis (X,Y) and floor level\n",
        "utility_combo1_permutations = generate_permutations(param_axis, param_level)\n",
        "utility_combo1_len = len(utility_combo1_permutations)\n",
        "\n",
        "#utility load combo1 array defining all the combos we need for utility combo1\n",
        "utility_combo1_array = [None] * utility_combo1_len\n",
        "\n",
        "for i in range(utility_combo1_len):\n",
        "  utility_combo1_array[i] = '\"ELF-EQ' + utility_combo1_permutations[i][0] + '-' + utility_combo1_permutations[i][1]+'\"'\n",
        "\n",
        "for x in range(utility_combo1_len):\n",
        "    utility_combo1.append('  COMBO  ' + utility_combo1_array[x] + '  TYPE ' + type)\n",
        "    for i in range(n):\n",
        "      if ((permutations[i][0] == utility_combo1_permutations[x][0]) and (permutations[i][3] == utility_combo1_permutations[x][1])):\n",
        "         utility_combo1.append( '  COMBO  ' + utility_combo1_array[x] + '  LOADCASE  ' + pattern_array[i] + '  SF 1 ')"
      ],
      "metadata": {
        "id": "_OM4R6WSEh-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear-Add all Levels while adding Fpx scale factor to single floors\n",
        "\n",
        "utility_combo2 = [];\n",
        "\n",
        "type = '\"Linear Add\"'\n",
        "\n",
        "#permutations required for utility combo 2 capturing Fpx scaling for single floor for each direction (X,Y) with linear add of all floors\n",
        "utility_combo2_permutations = generate_permutations(param_axis, param_level)\n",
        "utility_combo2_len = len(utility_combo2_permutations)\n",
        "\n",
        "#utility load combo2 array defining all the combos we need for utility combo1\n",
        "utility_combo2_array = [None] * utility_combo2_len\n",
        "\n",
        "for i in range(utility_combo2_len):\n",
        "  utility_combo2_array[i] = '\"ELF-EQ' + utility_combo2_permutations[i][0] + '-Fpx-' + utility_combo2_permutations[i][1]+'\"'\n",
        "\n",
        "#create utility laod combo2 array\n",
        "for x in range(utility_combo2_len):\n",
        "    utility_combo2.append('  COMBO  ' + utility_combo2_array[x] + '  TYPE ' + type)\n",
        "\n",
        "    #Fpx scale factor depending on axis of force (X,Y)\n",
        "    SF = Fpx_SF.iloc[Fpx_SF.index.get_loc(utility_combo2_permutations[x][0]), Fpx_SF.columns.get_loc(utility_combo2_permutations[x][1])]\n",
        "\n",
        "    for i in range(utility_combo2_len):\n",
        "\n",
        "      #apply Fpx only if the floor matches the loop increment\n",
        "      if ((utility_combo1_permutations[i][0] == utility_combo2_permutations[x][0]) and (utility_combo1_permutations[i][1] == utility_combo2_permutations[x][1])):\n",
        "         utility_combo2.append( '  COMBO  ' + utility_combo2_array[x] + '  LOADCOMBO  ' + utility_combo1_array[i] + '  SF ' + str(SF) )\n",
        "\n",
        "      elif (utility_combo1_permutations[i][0] == utility_combo2_permutations[x][0]):\n",
        "         utility_combo2.append( '  COMBO  ' + utility_combo2_array[x] + '  LOADCOMBO  ' + utility_combo1_array[i] + '  SF 1 ' )\n"
      ],
      "metadata": {
        "id": "aJQXHhVe-Olz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Envelope by direction (X,Y) for all floors in order to get final\n",
        "\n",
        "utility_combo3 = [];\n",
        "\n",
        "type = '\"Envelope\"'\n",
        "\n",
        "#permutations required for utility combo 2 capturing Fpx scaling for single floor for each direction (X,Y) with linear add of all floors\n",
        "utility_combo3_permutations = generate_permutations(param_axis)\n",
        "utility_combo3_len = len(utility_combo3_permutations)\n",
        "\n",
        "#utility load combo3 array defining all the combos we need for utility combo1\n",
        "utility_combo3_array = [None] * utility_combo3_len\n",
        "\n",
        "for i in range(utility_combo3_len):\n",
        "  utility_combo3_array[i] = '\"ELF-EQ' + utility_combo3_permutations[i][0] +'\"'\n",
        "\n",
        "#create utility laod combo2 array\n",
        "for x in range(utility_combo3_len):\n",
        "  utility_combo3.append('  COMBO  ' + utility_combo3_array[x] + '  TYPE ' + type)\n",
        "\n",
        "  for i in range(utility_combo2_len):\n",
        "\n",
        "    #apply Fpx only if the floor matches the loop increment\n",
        "    if ((utility_combo2_permutations[i][0] == utility_combo3_permutations[x][0])):\n",
        "       utility_combo3.append( '  COMBO  ' + utility_combo3_array[x] + '  LOADCOMBO  ' + utility_combo2_array[i] +'  SF 1 ' )"
      ],
      "metadata": {
        "id": "OKqBj4qVKEmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_combo_array = utility_combo1 + utility_combo2 + utility_combo3"
      ],
      "metadata": {
        "id": "pDYGSfhdJKuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write to a .txt file (temporary)"
      ],
      "metadata": {
        "id": "xku_WsS-XuiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"testloads2.txt\", \"w\") as file:\n",
        "    for item in load_pattern_array:\n",
        "        file.write(item + '\\n')  # Write each item on a new line\n",
        "\n",
        "with open(\"testloads2.txt\", \"a\") as file:\n",
        "    for item in load_case_array:\n",
        "        file.write(item + '\\n')  # Write each item on a new line\n",
        "\n",
        "with open(\"testloads2.txt\", \"a\") as file:\n",
        "    for item in load_combo_array:\n",
        "        file.write(item + '\\n')  # Write each item on a new line\n"
      ],
      "metadata": {
        "id": "drat8y5xX7zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Insert Load Patterns, Cases, and Combos into .e2k file"
      ],
      "metadata": {
        "id": "xo_FPu8Kn4eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #read in .e2k file as target filepath\n",
        "# #identify where the text file ends\n",
        "# #append the requisite lines\n",
        "\n",
        "# #taken from testquantities workbook:\n",
        "# def file_indexes(e2k_file):\n",
        "#   with open(e2k_file, 'r') as file:\n",
        "#       # Initialize lists to store data\n",
        "#       dollar_lines = []\n",
        "#       line_indices = []\n",
        "#       line_lengths = []\n",
        "\n",
        "#       # Initialize variables\n",
        "#       after_dollar = False\n",
        "#       line_count = 0\n",
        "\n",
        "#       # Read each line in the file\n",
        "#       for idx, line in enumerate(file):\n",
        "#           # Skip the line \"$ END OF MODEL FILE\"\n",
        "#           if line.strip() == \"$ END OF MODEL FILE\":\n",
        "#               continue\n",
        "\n",
        "#           # Check if the line starts with '$'\n",
        "#           if line.startswith('$'):\n",
        "#               # Set the flag to True indicating that we are currently reading lines after the '$' line\n",
        "#               after_dollar = True\n",
        "#               # Append the line to the list of dollar lines\n",
        "#               dollar_lines.append(line.strip())\n",
        "#               # Append the index of the line\n",
        "#               line_indices.append(idx)\n",
        "#               # Reset line count\n",
        "#               line_count = 0\n",
        "#           # Check if we are currently reading lines after the '$' line and we encounter a blank line\n",
        "#           elif after_dollar and line.strip() == '':\n",
        "#               # If so, append the line count to the list of line lengths\n",
        "#               line_lengths.append(line_count)\n",
        "#               # Reset the flag indicating that we are no longer reading lines after the '$' line\n",
        "#               after_dollar = False\n",
        "#           # Increment the line count if we are currently reading lines after the '$' line\n",
        "#           elif after_dollar:\n",
        "#               line_count += 1\n",
        "\n",
        "#   # Create a DataFrame from the collected data\n",
        "#   Data_Indexes = pd.DataFrame({'Line': dollar_lines, 'Index': line_indices, 'Length': line_lengths})\n",
        "#   return Data_Indexes\n"
      ],
      "metadata": {
        "id": "akqg2ahFn9gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(e2k_file, 'r') as file:\n",
        "#   e2k_lines = file.readlines()\n",
        "\n",
        "#   load_patterns = '$ LOAD PATTERNS\\n'\n",
        "\n",
        "#   for text in e2k_lines:\n",
        "#     if load_patterns in text:\n",
        "#       line_index = e2k_lines.index(load_patterns)\n",
        "#       e2k_lines.insert(line_index+1, load_pattern_array)\n",
        "\n",
        "#   load_cases = '$ LOAD CASES\\n'\n",
        "\n",
        "#   for text in e2k_lines:\n",
        "#     if load_cases in text:\n",
        "#       line_index = e2k_lines.index(load_cases)\n",
        "#       e2k_lines.insert(line_index+1, load_case_array)\n",
        "\n",
        "#   load_combos = '$ LOAD COMBOS\\n'\n",
        "\n",
        "#   for text in e2k_lines:\n",
        "#     if load_combos in text:\n",
        "#       line_index = e2k_lines.index(load_combos)\n",
        "#       e2k_lines.insert(line_index+1, load_combo_array)\n",
        "\n",
        "# with open('newfile4.txt', 'w') as file:\n",
        "#     for line in e2k_lines:\n",
        "#         file.write(str(line))"
      ],
      "metadata": {
        "id": "J92g3STWDRth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = file_indexes(e2k_file)\n",
        "# data"
      ],
      "metadata": {
        "id": "Yyq4xzFhWvbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #function for writing back into the same text file\n",
        "\n",
        "# # def insert_lines_at(file_path, line_number, new_lines):\n",
        "# # Read the existing lines from the file\n",
        "# with open(file_path, 'r') as file:\n",
        "#     lines = file.readlines()\n",
        "\n",
        "# # Insert new lines at the specified line number\n",
        "# if 0 <= line_number <= len(lines):\n",
        "#     for i, line in enumerate(new_lines):\n",
        "#         lines.insert(line_number + i, line + '\\n')\n",
        "# # else:\n",
        "# #     print(f\"Error: The file has only {len(lines)} lines.\")\n",
        "# #     return\n",
        "\n",
        "# # Write the updated lines back to the file\n",
        "# with open(file_path, 'w') as file:\n",
        "#     file.writelines(lines)\n"
      ],
      "metadata": {
        "id": "w_iIzqv_B_iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Insert Load Patterns\n",
        "\n",
        "# Data_Indexes = file_indexes()\n",
        "# header_name = '$ LOAD PATTERNS'\n",
        "# line_insert = Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][0] + Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][1]+1\n",
        "# insert_lines_at(e2k_file, line_insert, load_case_array)\n",
        "\n",
        "# Data_Indexes = file_indexes()\n",
        "# #Insert Load Cases\n",
        "# header_name = '$ LOAD CASES'\n",
        "# line_insert = Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][0] + Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][1]+1\n",
        "# insert_lines_at(e2k_file, line_insert, load_case_array)\n",
        "\n",
        "# Data_Indexes = file_indexes()\n",
        "# #Insert Load Combos\n",
        "# header_name = '$ LOAD COMBINATIONS'\n",
        "# line_insert = Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][0] + Data_Indexes.loc[Data_Indexes['Line']==header_name, ['Index','Length']].iloc[0][1]+1\n",
        "# insert_lines_at(e2k_file, line_insert, load_case_array)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gyAww0VCgkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pRV748teAgap"
      }
    }
  ]
}